{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2994f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli login --token $LoginToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53756051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install datasets\n",
    "# !conda install huggingface_hub\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# !pip install scikit-image\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f5261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "''' \n",
    "Creates a class object MyDataset\n",
    "************\n",
    "__init__\n",
    "  This function will load a part of the ImageNet-1K dataset specified in the \n",
    "  class caller.\n",
    "  Options are: train, validation, and test.\n",
    "************\n",
    "__getitem__\n",
    "  This function will return two variables containing each a variable present \n",
    "  in the original ImageNet-1K dataset.\n",
    "  Before returning these variables, it will split up transforms the image to a\n",
    "  fixed resolution of 256 by 256 pixels.\n",
    "  \n",
    "Returns: data, label\n",
    "************\n",
    "__len__\n",
    "Returns: the length of the loaded dataset.\n",
    "'''\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        # Loads the dataset that needs to be transformed\n",
    "        self.dataset = load_dataset(\"imagenet-1k\", split=f\"{dataset}\")\n",
    "\n",
    "#         # define your transform pipline\n",
    "#         transform = transforms.Compose([\n",
    "#             transforms.Resize((256, 256)),\n",
    "#             Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),  # Convert grayscale to RGB\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "        \n",
    "       # load your dataset (how every you want, this example has the dataset stored in a json file            \n",
    "#         self.dataset = load_dataset(\"imagenet-1k\", split=f\"{dataset}[:1%]\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Sample row idx from the loaded dataset\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # Split up sample example into an image and label variable\n",
    "        data, label = sample['image'], sample['label']\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize to size 256x256\n",
    "            Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),  # Convert all images to RGB format\n",
    "            transforms.ToTensor(),  # Transform image to Tensor object\n",
    "        ])\n",
    "        \n",
    "        return transform(data), torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b7ea",
   "metadata": {},
   "source": [
    "### Setting up Multi-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165ddebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da582ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deCNN(Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        # call the parent constructor\n",
    "        super(deCNN, self).__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=50 * 61 * 61, out_features=500)  # Update in_features\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfb119",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5560e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba1986fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b28c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n",
    "#     help=\"path to output trained model\")\n",
    "# ap.add_argument(\"-p\", \"--plot\", type=str, required=True,\n",
    "#     help=\"path to output loss/accuracy plot\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d8c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82c2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     Lambda(lambda x: x.convert(\"RGB\") if x.mode != \"RGB\" else x),  # Convert grayscale to RGB\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Define a function to apply the transformation to the dataset\n",
    "# def transform_dataset(dataset):\n",
    "#     transformed_data = []\n",
    "#     for item in dataset:\n",
    "#         # Apply transformation\n",
    "#         img = transform(item[\"image\"])\n",
    "#         transformed_data.append({\"image\": img, \"label\": item[\"label\"]})\n",
    "#     return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6abad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the transforms directly to the datasets\n",
    "# transformed_training_data = transform_dataset(training_data)\n",
    "# transformed_validation_data = transform_dataset(validation_data)\n",
    "# transformed_test_data = transform_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ed25228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define train and val splits if validation set is not present\n",
    "# if validation_data == False:\n",
    "#     TRAIN_SPLIT = 0.75\n",
    "#     VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "#     # Calculate the train/validation split\n",
    "#     print(\"[INFO] generating the train/validation split...\")\n",
    "#     numTrainSamples = int(len(trainData) * TRAIN_SPLIT)\n",
    "#     numValSamples = int(len(trainData) * VAL_SPLIT)\n",
    "#     (trainData, valData) = random_split(trainData,\n",
    "#         [numTrainSamples, numValSamples],\n",
    "#         generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79147e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6f37a5e7545838afa24ae85915cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/85.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagenet-1k (C:/Users/rdede/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/09dbb3153f1ac686bac1f40d24f307c383b383bc171f2df5d9e91c1ad57455b9)\n",
      "Found cached dataset imagenet-1k (C:/Users/rdede/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/09dbb3153f1ac686bac1f40d24f307c383b383bc171f2df5d9e91c1ad57455b9)\n",
      "Found cached dataset imagenet-1k (C:/Users/rdede/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/09dbb3153f1ac686bac1f40d24f307c383b383bc171f2df5d9e91c1ad57455b9)\n"
     ]
    }
   ],
   "source": [
    "train_set = MyDataset('train')\n",
    "val_set = MyDataset('validation')\n",
    "test_set = MyDataset('test')\n",
    "\n",
    "# initialize the train, validation, and test data loaders\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valDataLoader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_set) // BATCH_SIZE\n",
    "valSteps = len(val_set) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4a61be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the deCNN model...\n"
     ]
    }
   ],
   "source": [
    "# initialize the deCNN model\n",
    "print(\"[INFO] initializing the deCNN model...\")\n",
    "model = deCNN(\n",
    "    numChannels=3,\n",
    "    classes=1000).to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c5e3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model checkpoint saving location\n",
    "PATH = 'models/12-20-2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cab19866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "def save_ckp(state, checkpoint_dir, epoch_nr):\n",
    "    f_path = checkpoint_dir + f'/epoch_{epoch_nr}.pt'\n",
    "    torch.save(state, f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "170914a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading the network from checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "print(\"[INFO] Loading the network from checkpoint\")\n",
    "files = sorted(glob.glob(f\"{PATH}/*\"), key=os.path.getctime, reverse=True)\n",
    "checkpoint = torch.load(files[0])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer'])\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e727150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the deCNN network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2836/5005 [7:07:34<5:53:08,  9.77s/batch] "
     ]
    }
   ],
   "source": [
    "###### measure how long training is going to take\n",
    "print(\"[INFO] training the deCNN network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "# Create a GradScaler for mixed-precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# loop over our epochs\n",
    "for e in range(27, EPOCHS):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    # initialize the number of correct predictions in the training\n",
    "    # and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "\n",
    "    # loop over the training set\n",
    "    with tqdm(trainDataLoader, unit=\"batch\") as tepoch:\n",
    "        for x, y in tepoch:\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "\n",
    "            # perform a forward pass and calculate the training loss\n",
    "            pred = model(x)\n",
    "            loss = lossFn(pred, y)\n",
    "\n",
    "            # zero out the gradients, perform the backpropagation step,\n",
    "            # and update the weights\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # add the loss to the total training loss so far and\n",
    "            # calculate the number of correct predictions\n",
    "            totalTrainLoss += loss\n",
    "            trainCorrect += (pred.argmax(1) == y).type(\n",
    "                torch.float).sum().item()\n",
    "\n",
    "    # switch off autograd for evaluation\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        # loop over the validation set\n",
    "        with tqdm(valDataLoader, unit=\"batch\") as vepoch:\n",
    "            for (x, y) in vepoch:\n",
    "                # send the input to the device\n",
    "                (x, y) = (x.to(device), y.to(device))\n",
    "                # make the predictions and calculate the validation loss\n",
    "                pred = model(x)\n",
    "                totalValLoss += lossFn(pred, y)\n",
    "                # calculate the number of correct predictions\n",
    "                valCorrect += (pred.argmax(1) == y).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    # calculate the training and validation accuracy\n",
    "    trainCorrect = trainCorrect / len(train_set)\n",
    "    valCorrect = valCorrect / len(val_set)\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss)\n",
    "    H[\"train_acc\"].append(trainCorrect)\n",
    "    H[\"val_loss\"].append(avgValLoss)\n",
    "    H[\"val_acc\"].append(valCorrect)\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "        avgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "        avgValLoss, valCorrect))\n",
    "\n",
    "    # Save the epoch model parameters\n",
    "    checkpoint = {\n",
    "        'epoch': e + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': opt.state_dict()\n",
    "    }\n",
    "\n",
    "    save_ckp(checkpoint, PATH , e)\n",
    "\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "    endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for batch in valDataLoader:\n",
    "        x = batch[\"image\"].to(device)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "\n",
    "# generate a classification report\n",
    "print(classification_report(validation_data['label'], np.array(preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
